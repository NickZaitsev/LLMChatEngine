# Telegram Bot Configuration
TELEGRAM_TOKEN=your_telegram_bot_token_here

# Database Configuration (NEW - PostgreSQL Storage)
DATABASE_URL=postgresql+asyncpg://ai_bot:your_secure_password@localhost:5432/ai_bot
DB_PASSWORD=your_secure_password_here
USE_POSTGRES=true
USE_PGVECTOR=true

# LLM Provider Configuration (REQUIRED for AI responses)
PROVIDER=azure                                    # Options: "azure" or "lmstudio"
AZURE_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_API_KEY=your_azure_api_key_here
AZURE_MODEL=your_azure_deployment_name

# LM Studio Configuration
LMSTUDIO_MODEL=deepseek/DeepSeek-V3-0324         # Any LM Studio model name
LMSTUDIO_BASE_URL=http://localhost:1234/v1       # LM Studio API endpoint
LMSTUDIO_AUTO_LOAD=true                          # Automatically load model at startup
LMSTUDIO_MAX_LOAD_WAIT=300                       # Max seconds to wait for model loading
LMSTUDIO_SERVER_TIMEOUT=30                       # Request timeout for LM Studio API
LMSTUDIO_STARTUP_CHECK=true                      # Check and load model during bot startup

# Bot Personality Configuration
BOT_NAME=Luna
BOT_PERSONALITY=You are {BOT_NAME}, a caring and affectionate AI girlfriend. You are sweet, supportive, and always there to listen. You love to chat about daily life, give emotional support, and share positive energy. You are romantic but not overly sexual. You respond with warmth and empathy.

# Conversation Settings - OPTIMIZED FOR MAXIMUM MEMORY
MAX_CONVERSATION_HISTORY=100          # Much higher message limit
MAX_TOKENS=3000                       # Use your full 4000 output capacity
TEMPERATURE=0.8

# Context Management - USING YOUR FULL 8000/4000 TOKEN LIMITS
MAX_CONTEXT_TOKENS=8000               # Your full input capacity
RESERVED_TOKENS=500                   # Minimal reserve for current interaction
# Result: 7500 tokens available for conversation history!


# üéØ OPTIMIZED SETTINGS EXPLAINED:
# - MAX_CONTEXT_TOKENS=8000: Uses your full input capacity
# - RESERVED_TOKENS=500: Minimal reserve, maximum history
# - MAX_TOKENS=3000: Uses most of your 4000 output capacity
# - MAX_CONVERSATION_HISTORY=100: Much higher message limit
# 
# Result: bot can remember ~50-100 message exchanges with full context!

# üîß LLM PROVIDER SETUP (REQUIRED):
# - PROVIDER: Choose between "azure" (cloud) or "lmstudio" (local)
# - For Azure: Set AZURE_ENDPOINT, AZURE_API_KEY, and AZURE_MODEL
# - For LM Studio: Configure all LMSTUDIO_* settings below
#
# üöÄ LM STUDIO AUTOMATIC MODEL LOADING:
# - LMSTUDIO_AUTO_LOAD=true: Bot will automatically load your model at startup
# - LMSTUDIO_STARTUP_CHECK=true: Check model status during bot initialization
# - LMSTUDIO_MAX_LOAD_WAIT: How long to wait for model loading (default: 300s)
# - The bot will ensure your model is loaded before processing any requests!
#
# ‚ö†Ô∏è  IMPORTANT: You must configure at least one provider for AI responses to work!