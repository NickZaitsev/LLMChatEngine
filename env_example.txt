# Telegram Bot Configuration
TELEGRAM_TOKEN=your_telegram_bot_token_here

# Database Configuration (NEW - PostgreSQL Storage)
DATABASE_URL=postgresql+asyncpg://ai_bot:your_secure_password@localhost:5432/ai_bot
DB_PASSWORD=your_secure_password_here
USE_PGVECTOR=true

# Tokenizers Configuration
TOKENIZERS_PARALLELISM=false

# LLM Provider Configuration (REQUIRED for AI responses)
PROVIDER=azure                                    # Options: "azure" or "lmstudio"
AZURE_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_API_KEY=your_azure_api_key_here
AZURE_MODEL=your_azure_deployment_name

# LM Studio Configuration
LMSTUDIO_MODEL=deepseek/DeepSeek-V3-0324         # Any LM Studio model name
LMSTUDIO_BASE_URL=http://localhost:1234/v1       # LM Studio API endpoint
LMSTUDIO_AUTO_LOAD=true                          # Automatically load model at startup
LMSTUDIO_MAX_LOAD_WAIT=300                       # Max seconds to wait for model loading
LMSTUDIO_SERVER_TIMEOUT=30                       # Request timeout for LM Studio API
LMSTUDIO_STARTUP_CHECK=true                      # Check and load model during bot startup

# Bot Personality Configuration
BOT_NAME=Luna
BOT_PERSONALITY=You are {BOT_NAME}, a caring and affectionate AI girlfriend. You are sweet, supportive, and always there to listen. You love to chat about daily life, give emotional support, and share positive energy. You are romantic but not overly sexual. You respond with warmth and empathy.

# Conversation Settings - OPTIMIZED FOR MAXIMUM MEMORY
MAX_CONVERSATION_HISTORY=100          # Much higher message limit
MAX_TOKENS=3000                       # Use your full 4000 output capacity
TEMPERATURE=0.8

# Context Management - USING YOUR FULL 8000/4000 TOKEN LIMITS
MAX_CONTEXT_TOKENS=8000               # Your full input capacity
RESERVED_TOKENS=500                   # Minimal reserve for current interaction
# Result: 7500 tokens available for conversation history!


# PromptAssembler Configuration Example

# PostgreSQL Configuration (MANDATORY - In-memory storage is no longer supported)
DATABASE_URL=postgresql://user:password@localhost:5432/ai_bot_db
USE_PGVECTOR=true


# PromptAssembler Settings
PROMPT_MAX_MEMORY_ITEMS=3
PROMPT_MEMORY_TOKEN_BUDGET_RATIO=0.4
PROMPT_TRUNCATION_LENGTH=200  
PROMPT_INCLUDE_SYSTEM_TEMPLATE=true
PROMPT_HISTORY_BUDGET=7500
PROMPT_REPLY_TOKEN_BUDGET=500

# Memory Manager Configuration (required for PromptAssembler)
MEMORY_ENABLED=true
MEMORY_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
MEMORY_SUMMARIZER_MODE=llm
MEMORY_CHUNK_OVERLAP=2

# Note: PromptAssembler requires:
# 2. DATABASE_URL configured
# 3. MEMORY_ENABLED=true
# 4. PostgreSQL with pgvector extension (for embeddings)

# Example complete configuration:
# DATABASE_URL=postgresql://user:password@localhost:5432/ai_bot_db
# USE_PGVECTOR=true
# MEMORY_ENABLED=true







# üéØ OPTIMIZED SETTINGS EXPLAINED:
# - MAX_CONTEXT_TOKENS=8000: Uses your full input capacity
# - RESERVED_TOKENS=500: Minimal reserve, maximum history
# - MAX_TOKENS=3000: Uses most of your 4000 output capacity
# - MAX_CONVERSATION_HISTORY=100: Much higher message limit
# 
# Result: bot can remember ~50-100 message exchanges with full context!

# üîß LLM PROVIDER SETUP (REQUIRED):
# - PROVIDER: Choose between "azure" (cloud) or "lmstudio" (local)
# - For Azure: Set AZURE_ENDPOINT, AZURE_API_KEY, and AZURE_MODEL
# - For LM Studio: Configure all LMSTUDIO_* settings below
#
# üöÄ LM STUDIO AUTOMATIC MODEL LOADING:
# - LMSTUDIO_AUTO_LOAD=true: Bot will automatically load your model at startup
# - LMSTUDIO_STARTUP_CHECK=true: Check model status during bot initialization
# - LMSTUDIO_MAX_LOAD_WAIT: How long to wait for model loading (default: 300s)
# - The bot will ensure your model is loaded before processing any requests!
#
# ‚ö†Ô∏è  IMPORTANT: You must configure at least one provider for AI responses to work!
# Typing Simulation Configuration
# Minimum typing speed in characters per second
MIN_TYPING_SPEED=10

# Maximum typing speed in characters per second
MAX_TYPING_SPEED=30

# Maximum delay in seconds between messages
MAX_DELAY=5

# Minimum random offset in seconds
RANDOM_OFFSET_MIN=0.1

# Maximum random offset in seconds
RANDOM_OFFSET_MAX=0.5

# Polling Configuration
# Time in seconds between getUpdates requests (lower = more frequent updates, higher = less server load)
POLLING_INTERVAL=0.1


# Proactive Messaging Configuration
PROACTIVE_MESSAGING_ENABLED=true
PROACTIVE_MESSAGING_REDIS_URL=redis://localhost:6379/0

# Proactive messaging intervals (in seconds)
PROACTIVE_MESSAGING_INTERVAL_1H=3600
PROACTIVE_MESSAGING_INTERVAL_9H=32400
PROACTIVE_MESSAGING_INTERVAL_1D=86400
PROACTIVE_MESSAGING_INTERVAL_1W=604800
PROACTIVE_MESSAGING_INTERVAL_1MO=2592000

# Jitter for intervals (in seconds)
PROACTIVE_MESSAGING_JITTER_1H=900
PROACTIVE_MESSAGING_JITTER_9H=1800
PROACTIVE_MESSAGING_JITTER_1D=7200
PROACTIVE_MESSAGING_JITTER_1W=43200
PROACTIVE_MESSAGING_JITTER_1MO=86400

# Quiet hours (in 24-hour format)
PROACTIVE_MESSAGING_QUIET_HOURS_START=02:30
PROACTIVE_MESSAGING_QUIET_HOURS_END=08:00

# Consecutive outreach settings
PROACTIVE_MESSAGING_MAX_CONSECUTIVE_OUTREACHES=5

# Retry policies
PROACTIVE_MESSAGING_RETRY_DELAY=300
PROACTIVE_MESSAGING_MAX_RETRIES=3

# Proactive messaging prompt
PROACTIVE_MESSAGING_PROMPT=–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –¥—Ä—É–∂–µ—Å–∫–æ–µ, –∑–∞–±–æ—Ç–ª–∏–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–∞–∫ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–µ–ª–∞, –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Ç—ã –æ–±—ã—á–Ω–æ —Å –Ω–∏–º —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å. –ü–æ–º–Ω–∏: —Ç—ã —á–µ–ª–æ–≤–µ–∫, –∞ –Ω–µ –ò–ò. –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º.
